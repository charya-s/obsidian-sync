This is the basic building block of neural networks. These are analogous to "neurons" in a brain.

![[Pasted image 20240922001111.png | center]]


Neural networks such as the one above use the following equation for forward propagation.
$$
\hat{y} = g \left( \sum_{i=1}^{m}{x_{i} w_{i}} \right)
$$
Where $\hat{y}$ is the output of the perception, $g$ is the [[Activation Function]],  $m$ is the total number of inputs, $x_{i}$ is the input and $w$ is the weight of the current input.



