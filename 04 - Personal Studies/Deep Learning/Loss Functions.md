$$
\begin{gather*}
J(W) = \frac{1}{n}\sum_{i=1}^{n}{L(\hat{y}^{(i)}, y^{(i)})} = \frac{1}{n}\sum_{i=1}^{n}{L(f(x^{(i)};W), y^{(i)})}
\end{gather*}
$$

where $J(W)$ is the total loss across the dataset, $n$ is the total number of datapoints, $L$ is the loss function, $f(x^{(i)};W)$ is the predicted output and $y_{(i)}$ is the expected output.

Various loss functions, $L$, can be used depending on the application of the model. 

---
### Mean Squared Error


---
### Root Mean Squared Error


---
### Binary Cross Entropy Loss



---
### Multi-Class Cross-Entropy Loss



---
### Sparse Multi-Class Cross-Entropy Loss



---
### Kullback Leibler (KL) Divergence Loss



---
